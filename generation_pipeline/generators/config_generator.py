"""
Config Generator - Generates StudyConfig classes from extraction results using LLM
"""

import json
import re
import sys
from pathlib import Path
from typing import Dict, Any, Optional

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from src.llm.factory import get_client
from generation_pipeline.utils.pdf_extractor import extract_pdf_text

PDF_TEXT_MAX_CHARS = 400000


class ConfigGenerator:
    """Generate StudyConfig classes from extraction results using LLM"""

    def __init__(
        self,
        provider: str = "gemini",
        model: str = "models/gemini-3-flash-preview",
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
    ):
        """
        Initialize config generator.

        Args:
            provider: One of gemini, openai, anthropic, xai, openrouter
            model: Model name
            api_key: Optional API key
            api_base: Optional API base URL
        """
        self.client = get_client(provider=provider, model=model, api_key=api_key, api_base=api_base)
    
    def generate(
        self,
        extraction_result: Dict[str, Any],
        study_id: str,
        output_path: Path,
        pdf_path: Optional[Path] = None,
        study_dir: Optional[Path] = None
    ) -> Path:
        """
        Generate StudyConfig class file using a combination of Template and LLM.
        """
        studies = extraction_result.get('studies', [])
        if not studies:
            raise ValueError("No studies found in extraction result")
        
        # Auto-find PDF if not provided
        if pdf_path is None and study_dir:
            pdf_files = list(study_dir.glob("*.pdf"))
            if pdf_files:
                pdf_path = pdf_files[0]
        
        # Load all study data for context
        study_context = {}
        material_previews = []
        if study_dir:
            for json_file in ["metadata.json", "specification.json", "ground_truth.json"]:
                p = study_dir / json_file
                if p.exists():
                    try:
                        study_context[json_file] = json.loads(p.read_text(encoding='utf-8'))
                    except: pass
        
            # Get material previews (all filenames + first few lines)
            materials_dir = study_dir / "materials"
            if materials_dir.exists():
                for json_file in materials_dir.glob("*.json"):
                    try:
                        content = json_file.read_text(encoding='utf-8')
                        preview = "\n".join(content.splitlines()[:20])
                        material_previews.append(f"FILE: {json_file.name}\n{preview}\n...")
                    except: pass
        
        material_context = "\n\n".join(material_previews)
        
        # 1. Ask LLM to generate ONLY the inner logic of the class
        prompt = self._build_logic_only_prompt(
            json.dumps(extraction_result, indent=2, ensure_ascii=False),
            study_id,
            json.dumps(study_context, indent=2, ensure_ascii=False),
            material_context
        )

        try:
            if pdf_path and pdf_path.exists():
                pdf_text = extract_pdf_text(pdf_path, max_chars=PDF_TEXT_MAX_CHARS)
                full_prompt = f"PDF content:\n\n{pdf_text}\n\n{prompt}"
                response = self.client.generate_content(prompt=full_prompt)
            else:
                response = self.client.generate_content(prompt=prompt)
        except Exception as e:
            raise RuntimeError(f"LLM Error: {e}")
        
        # 2. Extract logic from response
        logic_code = self._extract_code_from_response(response)
        
        # 3. Assemble final file using FIXED template
        class_name = f"Study{study_id.replace('_', '').capitalize()}Config"
        final_code = f"""import json
import re
import numpy as np
from scipy import stats
from pathlib import Path
from typing import Dict, Any, List, Optional

from src.core.study_config import BaseStudyConfig, StudyConfigRegistry
from src.agents.prompt_builder import PromptBuilder

{logic_code}
"""
        # Ensure the class name is correct and registered
        if f"class {class_name}" not in final_code:
            # Replace the placeholder class name if LLM used a generic one
            # Handle cases with and without inheritance
            final_code = re.sub(r"class StudyConfig\s*:", f"class {class_name}(BaseStudyConfig):", final_code)
            final_code = re.sub(r"class \w+Config\s*:", f"class {class_name}(BaseStudyConfig):", final_code)
            final_code = re.sub(r"class \w+Config\(BaseStudyConfig\)\s*:", f"class {class_name}(BaseStudyConfig):", final_code)

        if f'@StudyConfigRegistry.register("{study_id}")' not in final_code:
            final_code = final_code.replace(f"class {class_name}", f'@StudyConfigRegistry.register("{study_id}")\nclass {class_name}')

        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(final_code, encoding='utf-8')
        return output_path

    def _build_logic_only_prompt(self, extraction_summary, study_id, context_summary, material_context):
        # Use a template string and manual replacement to avoid f-string curly brace errors with JSON/Code
        template = """You are a Python expert for HumanStudyBench. Your task is to write the CORE LOGIC for `[[STUDY_ID]]_config.py`.

STUDY ID: [[STUDY_ID]]

### Core Principles
1. **Match the human experimental design exactly** - One trial per participant with all items (unless within-subjects explicitly requires multiple trials)
2. **Use class attributes** - `prompt_builder_class` and `PROMPT_VARIANT` must be class attributes, not instance attributes
3. **Never skip sub-studies** - If `n=0` in specification, use a default (e.g., `n=50`) to ensure all experiments run

### Available Methods (from BaseStudyConfig)
- `self.load_material(sub_id)` - Load material JSON (sub_id is filename without .json extension)
- `self.load_specification()` - Returns `{"participants": {"n": ..., "by_sub_study": {...}}, ...}`
- `self.load_ground_truth()` - Returns `{"studies": [{"findings": [...]}], ...}`
- `self.extract_numeric(text)`, `self.extract_choice(text, options)` - Parse responses

### Note on Findings
- The study's `metadata.json` contains a `findings` array with finding-level weights (used for evaluation aggregation)
- Each finding has a `finding_id` that matches the `finding_id` in `ground_truth.json`
- This information is primarily used by evaluators, not config generation

### EXTRACTION SUMMARY (Goal)
[[EXTRACTION_SUMMARY]]

### MATERIALS (Context)
[[MATERIAL_CONTEXT]]

### Working Examples

**Example 1: Simple Study (study_001)**
```python
class CustomPromptBuilder(PromptBuilder):
    def __init__(self, study_path: Path):
        super().__init__(study_path)
    
    def build_trial_prompt(self, trial_metadata):
        # Note: System prompt is now handled separately by SystemPromptRegistry
        # This method only builds the task/trial content
        items = trial_metadata.get("items", [])
        
        prompt = ""
        
        # Add task context introduction (similar to study_003/004)
        prompt += "You are participating in a psychology study on decision-making. Please read the following instructions and provide your responses.\n\n"
        
        # Build questions with Q indices
        q_counter = 1
        for item in items:
            prompt += f"Q{q_counter} (answer with letter: A or B): {item['question']}\n"
            item["q_idx"] = q_counter
            q_counter += 1
        
        # RESPONSE_SPEC
        prompt += "\nRESPONSE_SPEC: Output Q1=<A/B>, Q2=<A/B>, etc.\n"
        return prompt

@StudyConfigRegistry.register("study_001")
class StudyStudy001Config(BaseStudyConfig):
    prompt_builder_class = CustomPromptBuilder  # Class attribute
    PROMPT_VARIANT = "v3"  # Class attribute
    
    def __init__(self, study_path: Path, specification: Dict[str, Any]):
        super().__init__(study_path, specification)
    
    def create_trials(self, n_trials=None):
        trials = []
        material = self.load_material("study_1_hypothetical_stories")
        n = 80 if n_trials is None else n_trials
        
        for item in material["items"]:
            for _ in range(n):
                trials.append({
                    "sub_study_id": "study_1_hypothetical_stories",
                    "scenario_id": item["id"],
                    "scenario": item["id"],
                    "items": [item],
                    "profile": {"age": random.randint(18, 22), "gender": random.choice(["Male", "Female"])},
                    "variant": self.PROMPT_VARIANT
                })
        return trials
    
    def dump_prompts(self, output_dir):
        trials = self.create_trials(n_trials=1)
        for idx, trial in enumerate(trials):
            prompt = self.prompt_builder.build_trial_prompt(trial)  # Use self.prompt_builder
            with open(f"{output_dir}/study_001_trial_{idx}.txt", "w") as f:
                f.write(prompt)
```

**Example 2: Between-Subjects Design (study_002)**
```python
class CustomPromptBuilder(PromptBuilder):
    def __init__(self, study_path: Path):
        super().__init__(study_path)
    
    def build_trial_prompt(self, trial_metadata):
        # Note: System prompt is now handled separately by SystemPromptRegistry
        # This method only builds the task/trial content
        items = trial_metadata.get("items", [])
        
        prompt = ""
        
        # Add task context introduction
        prompt += "You are participating in a psychology study on judgment and decision-making. Please read the following instructions and provide your responses.\n\n"
        
        q_counter = 1
        
        for item in items:
            # For anchored studies, assign anchor type at participant level
            anchor_type = item.get("assigned_anchor_type")
            anchor_val = item.get("metadata", {}).get(f"{anchor_type}_anchor")
            
            prompt += f"Q{q_counter}.1 (A/B): Is value higher/lower than {anchor_val}?\n"
            prompt += f"Q{q_counter}.2 (number): Your estimate?\n"
            item["q_idx_choice"] = f"Q{q_counter}.1"
            item["q_idx_estimate"] = f"Q{q_counter}.2"
            q_counter += 1
        
        prompt += f"\nRESPONSE_SPEC: Q1.1=<A/B>, Q1.2=<number>, Q2.1=<A/B>, Q2.2=<number>\n"
        return prompt

@StudyConfigRegistry.register("study_002")
class StudyStudy002Config(BaseStudyConfig):
    prompt_builder_class = CustomPromptBuilder
    PROMPT_VARIANT = "v3"
    
    def __init__(self, study_path: Path, specification: Dict[str, Any]):
        super().__init__(study_path, specification)
    
    def create_trials(self, n_trials=None):
        trials = []
        material = self.load_material("exp_1_anchored_estimation")
        spec = self.specification
        n = spec["participants"]["by_sub_study"]["exp_1_anchored_estimation"]["n"]
        if n == 0:
            n = 50  # Default to ensure experiment runs
        
        for i in range(n):
            # Assign anchor type at PARTICIPANT level (all items get same anchor)
            assigned_anchor_type = random.choice(["low", "high"])
            assigned_items = []
            for item in material["items"]:
                item_copy = item.copy()
                item_copy["assigned_anchor_type"] = assigned_anchor_type
                assigned_items.append(item_copy)
            
            # ONE trial per participant with ALL items
            trials.append({
                "sub_study_id": "exp_1_anchored_estimation",
                "scenario_id": "exp_1_anchored_estimation",
                "scenario": "exp_1_anchored_estimation",
                "items": assigned_items,
                "profile": {"age": random.randint(18, 25), "gender": random.choice(["male", "female"])},
                "variant": self.PROMPT_VARIANT
            })
        return trials
```

### Your Task
Generate the complete `CustomPromptBuilder` and `StudyConfig` classes following these patterns:
- Use class attributes for `prompt_builder_class` and `PROMPT_VARIANT`
- Call `super().__init__()` in both `__init__` methods
- Create ONE trial per participant with ALL items (unless within-subjects design)
- Use `Qk=<value>` or `Qk.n=<value>` format for responses. Please specify the exact format expected in the RESPONSE_SPEC.
- Include RESPONSE_SPEC with exact format expected
- Use `self.prompt_builder` (not `self.prompt_builder_class()`) in `dump_prompts`
- If `n=0`, use default `n=50` to ensure experiments run

DO NOT write import statements or the registration decorator - these will be added automatically.

Generate the code now:
"""
        return template.replace("[[STUDY_ID]]", study_id)\
                       .replace("[[EXTRACTION_SUMMARY]]", extraction_summary)\
                       .replace("[[MATERIAL_CONTEXT]]", material_context)

    def _extract_code_from_response(self, response: str) -> str:
        """Extract Python code from LLM response"""
        response_text = response.strip()
        
        # Remove markdown code blocks if present
        if '```python' in response_text:
            response_text = response_text.split('```python')[1].split('```')[0].strip()
        elif '```' in response_text:
            response_text = response_text.split('```')[1].split('```')[0].strip()
        
        # Remove any leading/trailing text that's not code
        lines = response_text.split('\n')
        
        # Find first line that looks like Python (import, class, def, or docstring)
        start_idx = 0
        for i, line in enumerate(lines):
            stripped = line.strip()
            if (stripped.startswith('\"\"\"') or 
                stripped.startswith("'''") or
                stripped.startswith('import ') or
                stripped.startswith('from ') or
                stripped.startswith('class ') or
                stripped.startswith('def ') or
                stripped.startswith('@')):
                start_idx = i
                break
        
        # Find last line that's part of the code
        end_idx = len(lines)
        for i in range(len(lines) - 1, -1, -1):
            stripped = lines[i].strip()
            if stripped and not stripped.startswith('#'):
                end_idx = i + 1
                break
        
        code = '\n'.join(lines[start_idx:end_idx])
        
        if not code:
            print(f"Warning: Extracted code is empty! Raw response length: {len(response)}")
            # Fallback to raw response if extraction failed but response exists
            if response.strip():
                code = response.strip()
        
        return code
